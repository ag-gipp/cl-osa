S. 40 1.3.3 Agents with State [...] The idea is that we now consider agents that maintain state -- see Figure 1.3. These agents have some internal data stucture, which is typically used to record information about the environment state and history. Let I be the set of all internal states of the agent. An agent's decision making process, at least in part on this information. The perception function see for a state-based agent is unchanged, mapping environment states to percepts as before_ see: S->P S. 41 The action-selection function action is now defined as a mapping action: I -> A from internal states to actions. An additional function next is introduced, which maps an internal state and percept to an internal state: next: I x P -> I The behavior of a state-based agent can be summarized as follows. The agent starts in some initial state i_0. It then observes its environment state s, and generates a percept see(s). The internal state of the agent is then updated via the next function, becoming set to next(i_o, see(s)). The action is then performed, and the agent enters another cycle, perceiving the world via see, updating its internal state via next. and choosing an action to perform via action. It is worth observing that state-based agents as defined here are in fact no more powerful than the standard agents we introduced earlier. In fact, they identical in ther expressive power -- every state-based agent can be transformed into a standard agent that is behaviorally equivalent.