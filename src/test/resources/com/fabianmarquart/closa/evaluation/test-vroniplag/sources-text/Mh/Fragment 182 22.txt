Agents with State [...] and we shall now replace it by an equivalent, but somewhat more natural, scheme. The idea is that we now consider agents that maintain state -- see Figure 2.3. These agents have some internal data structure, which is typically used to record information about the environment state and history. Let $ \mathcal{I} $ be the set of all internal states of the agent. An agent's decision-making process is then based, at least in part on this information. The perception function $ \mathcal{}see $ for a state-based agent is unchanged, mapping environment states to percepts as before: $ \mathcal{}see : E \to Per $ [Seite 36] The action-selection function action is now defined as a mapping $ \mathcal{}action : I \to Ac $ from internal states to actions. An additional function next is introduced, which maps an internal state and percept to an internal state: $ \mathcal{}next : I \times P \to I. $ The behavior of a state-based agent can be summarized as follows. The agent starts in some initial state $ \mathcal{}i_0 $. It then observes its environment state $ \mathcal{}e $, and generates a percept $ \mathcal{}see(e) $.