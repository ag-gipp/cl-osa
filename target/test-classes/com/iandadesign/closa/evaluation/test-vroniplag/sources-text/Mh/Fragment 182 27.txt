[p. 35] Agents with State [...] The idea is that we now consider agents that maintain state -- see Figure 2.3. These agents have some internal data structure, which is typically used to record information about the environment state and history. Let $ \mathcal{I} $ be the set of all internal states of the agent. An agent's decision-making process is then based, at least in part on this information. The perception function $ see $ for a state-based agent is unchanged, mapping environment states to percepts as before: $ see : E \to Per $ [p. 36] Wooldridge 2002, S. 36 The action-selection function action is now defined as a mapping $ action : I \to Ac $ from internal states to actions. An additional function next is introduced, which maps an internal state and percept to an internal state: $ next : I \times P \to I $ The behavior of a state-based agent can be summarized as follows. The agent starts in some initial state $ i_0 $. It then observes its environment state $ e $, and generates a percept $ see(e) $. The internal state of the agent is then updated via the $ next<math> function, becoming set to <math>next(i_0, see(e)) $. The action is then performed, and the agent enters another cycle, perceiving the world via see, updating its internal state via next. and choosing an action to perform via action. It is worth observing that state-based agents as defined here are in fact no more powerful than the standard agents we introduced earlier. In fact, they identical in their expressive power -- every state-based agent can be transformed into a standard agent that is behaviourally equivalent.